{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import io\n",
    "import sys\n",
    "from typing import List, Dict, Any\n",
    "from openai import OpenAI  # Ensure you have installed the `openai` library\n",
    "from groq import Groq  # Assuming you have installed the `groq` library\n",
    "\n",
    "# Set your API keys here (ensure they are stored securely)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\") or \"your_groq_api_key\"\n",
    "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\") or \"your_openrouter_api_key\"\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\") or \"your_openai_api_key\"\n",
    "\n",
    "# Initialize clients\n",
    "groq_client = Groq(api_key=groq_api_key)\n",
    "\n",
    "openrouter_client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=openrouter_api_key\n",
    ")\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    base_url=\"https://api.openai.com/v1\",\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "def get_llm_response(client, prompt, openai_model=\"o1-preview\", json_mode=False):\n",
    "\n",
    "    if client == \"openai\":\n",
    "\n",
    "        kwargs = {\n",
    "            \"model\": openai_model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "        }\n",
    "\n",
    "        if json_mode:\n",
    "            kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "\n",
    "        response = openai_client.chat.completions.create(**kwargs)\n",
    "\n",
    "    elif client == \"groq\":\n",
    "\n",
    "        try:\n",
    "            models = [\"llama-3.1-8b-instant\", \"llama-3.1-70b-versatile\", \"llama3-70b-8192\", \"llama3-8b-8192\", \"gemma2-9b-it\"]\n",
    "\n",
    "            for model in models:\n",
    "\n",
    "                try:\n",
    "                    kwargs = {\n",
    "                        \"model\": model,\n",
    "                        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "                    }\n",
    "                    if json_mode:\n",
    "                        kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "\n",
    "                    response = groq_client.chat.completions.create(**kwargs)\n",
    "\n",
    "                    break\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "            kwargs = {\n",
    "                \"model\": \"meta-llama/llama-3.1-8b-instruct:free\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "            }\n",
    "\n",
    "            if json_mode:\n",
    "                kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "\n",
    "            response = openrouter_client.chat.completions.create(**kwargs)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid client: {client}\")\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def evaluate_responses(prompt, reasoning_prompt=False, openai_model=\"o1-preview\"):\n",
    "\n",
    "    if reasoning_prompt:\n",
    "        prompt = f\"{prompt}\\n\\n{reasoning_prompt}.\"\n",
    "\n",
    "    openai_response = get_llm_response(\"openai\", prompt, openai_model)\n",
    "    groq_response = get_llm_response(\"groq\", prompt)\n",
    "\n",
    "    print(f\"OpenAI Response: {openai_response}\")\n",
    "    print(f\"\\n\\nGroq Response: {groq_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def planner(user_query: str) -> List[str]:\n",
    "   prompt = f\"\"\"Given the user's query: '{user_query}', break down the query into as few subtasks as possible in order to answer the question.\n",
    "   Each subtask is either a calculation or reasoning step. Never duplicate a task.\n",
    "  \n",
    "   Here are the only 2 actions that can be taken for each subtask:\n",
    "       - generate_code: This action involves generating Python code and executing it in order to make a calculation or verification.\n",
    "       - reasoning: This action involves providing reasoning for what to do to complete the subtask.\n",
    "  \n",
    "   Each subtask should begin with either \"reasoning\" or \"generate_code\".\n",
    "\n",
    "   Keep in mind the overall goal of answering the user's query throughout the planning process.\n",
    "  \n",
    "   Return the result as a JSON list of strings, where each string is a subtask.\n",
    "  \n",
    "   Here is an example JSON response:\n",
    "  \n",
    "   {{\n",
    "       \"subtasks\": [\"Subtask 1\", \"Subtask 2\", \"Subtask 3\"]\n",
    "   }}\n",
    "   \"\"\"\n",
    "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
    "   print(response)\n",
    "   return response[\"subtasks\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoner(user_query: str, subtasks: List[str], current_subtask: str, memory: List[Dict[str, Any]]) -> str:\n",
    "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
    "  \n",
    "   Here are all the subtasks to complete in order to answer the user's query:\n",
    "   <subtasks>\n",
    "       {json.dumps(subtasks)}\n",
    "   </subtasks>\n",
    "  \n",
    "   Here is the short-term memory (result of previous subtasks):\n",
    "   <memory>\n",
    "       {json.dumps(memory)}\n",
    "   </memory>\n",
    "  \n",
    "   The current subtask to complete is:\n",
    "   <current_subtask>\n",
    "       {current_subtask}\n",
    "   </current_subtask>\n",
    "  \n",
    "   - Provide concise reasoning on how to execute the current subtask, considering previous results.\n",
    "   - Prioritize explicit details over assumed patterns\n",
    "   - Avoid unnecessary complications in problem-solving\n",
    "  \n",
    "   Return the result as a JSON object with 'reasoning' as a key.\n",
    "  \n",
    "   Example JSON response:\n",
    "   {{\n",
    "       \"reasoning\": \"2 sentences max on how to complete the current subtask.\"\n",
    "   }}\n",
    "   \"\"\"\n",
    "  \n",
    "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
    "   return response[\"reasoning\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actioner(user_query: str, subtasks: List[str], current_subtask: str, reasoning: str, memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
    "  \n",
    "   The subtasks are:\n",
    "   <subtasks>\n",
    "       {json.dumps(subtasks)}\n",
    "   </subtasks>\n",
    "  \n",
    "   The current subtask is:\n",
    "   <current_subtask>\n",
    "       {current_subtask}\n",
    "   </current_subtask>\n",
    "  \n",
    "   The reasoning for this subtask is:\n",
    "   <reasoning>\n",
    "       {reasoning}\n",
    "   </reasoning>\n",
    "  \n",
    "   Here is the short-term memory (result of previous subtasks):\n",
    "   <memory>\n",
    "       {json.dumps(memory)}\n",
    "   </memory>\n",
    "  \n",
    "   Determine the most appropriate action to take:\n",
    "       - If the task requires a calculation or verification through code, use the 'generate_code' action.\n",
    "       - If the task requires reasoning without code or calculations, use the 'reasoning' action.\n",
    "  \n",
    "   Consider the overall goal and previous results when determining the action.\n",
    "  \n",
    "   Return the result as a JSON object with 'action' and 'parameters' keys. The 'parameters' key should always be a dictionary with 'prompt' as a key.\n",
    "  \n",
    "   Example JSON responses:\n",
    "  \n",
    "   {{\n",
    "       \"action\": \"generate_code\",\n",
    "       \"parameters\": {{\"prompt\": \"Write a function to calculate the area of a circle.\"}}\n",
    "   }}\n",
    "  \n",
    "   {{\n",
    "       \"action\": \"reasoning\",\n",
    "       \"parameters\": {{\"prompt\": \"Explain how to complete the subtask.\"}}\n",
    "   }}\n",
    "   \"\"\"\n",
    "  \n",
    "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
    "   return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(user_query: str, subtasks: List[str], current_subtask: str, action_info: Dict[str, Any], execution_result: Dict[str, Any], memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
    "  \n",
    "   The subtasks to complete to answer the user's query are:\n",
    "   <subtasks>\n",
    "       {json.dumps(subtasks)}\n",
    "   </subtasks>\n",
    "  \n",
    "   The current subtask to complete is:\n",
    "   <current_subtask>\n",
    "       {current_subtask}\n",
    "   </current_subtask>\n",
    "  \n",
    "   The result of the current subtask is:\n",
    "   <result>\n",
    "       {action_info}\n",
    "   </result>\n",
    "  \n",
    "   The execution result of the current subtask is:\n",
    "   <execution_result>\n",
    "       {execution_result}\n",
    "   </execution_result>\n",
    "  \n",
    "   Here is the short-term memory (result of previous subtasks):\n",
    "   <memory>\n",
    "       {json.dumps(memory)}\n",
    "   </memory>\n",
    "\n",
    "   Evaluate if the result is a reasonable answer for the current subtask and makes sense in the context of the overall query.\n",
    "  \n",
    "   Return a JSON object with 'evaluation' (string) and 'retry' (boolean) keys.\n",
    "  \n",
    "   Example JSON response:\n",
    "   {{\n",
    "       \"evaluation\": \"The result is a reasonable answer for the current subtask.\",\n",
    "       \"retry\": false\n",
    "   }}\n",
    "   \"\"\"\n",
    "  \n",
    "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
    "   return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_answer_extractor(user_query: str, subtasks: List[str], memory: List[Dict[str, Any]]) -> str:\n",
    "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
    "  \n",
    "   The subtasks completed to answer the user's query are:\n",
    "   <subtasks>\n",
    "       {json.dumps(subtasks)}\n",
    "   </subtasks>\n",
    "  \n",
    "   The memory of the thought process (short-term memory) is:\n",
    "   <memory>\n",
    "       {json.dumps(memory)}\n",
    "   </memory>\n",
    "  \n",
    "   Extract the final answer that directly addresses the user's query, from the memory.\n",
    "   Provide only the essential information without unnecessary explanations.\n",
    "  \n",
    "   Return a JSON object with 'finalAnswer' as a key.\n",
    "  \n",
    "   Here is an example JSON response:\n",
    "   {{\n",
    "       \"finalAnswer\": \"The final answer to the user's query, addressing all aspects of the question, based on the memory provided\",\n",
    "   }}\n",
    "   \"\"\"\n",
    "      \n",
    "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
    "   return response[\"finalAnswer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_execute_code(prompt: str, user_query: str, memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "   code_generation_prompt = f\"\"\"\n",
    "  \n",
    "   Generate Python code to implement the following task: '{prompt}'\n",
    "  \n",
    "   Here is the overall goal of answering the user's query: '{user_query}'\n",
    "      \n",
    "   Keep in mind the results of the previous subtasks, and use them to complete the current subtask.\n",
    "   <memory>\n",
    "       {json.dumps(memory)}\n",
    "   </memory>\n",
    "\n",
    "   Here are the guidelines for generating the code:\n",
    "       - Return only the Python code, without any explanations or markdown formatting.\n",
    "       - The code should always print or return a value\n",
    "       - Don't include any backticks or code blocks in your response. Do not include ```python or ``` in your response, just give me the code.\n",
    "       - Do not ever use the input() function in your code, use defined values instead.\n",
    "       - Do not ever use NLP techniques in your code, such as importing nltk, spacy, or any other NLP library.\n",
    "       - Don't ever define a function in your code, just generate the code to execute the subtask.\n",
    "       - Don't ever provide the execution result in your response, just give me the code.\n",
    "       - If your code needs to import any libraries, do it within the code itself.\n",
    "       - The code should be self-contained and ready to execute on its own.\n",
    "       - Prioritize explicit details over assumed patterns\n",
    "       - Avoid unnecessary complications in problem-solving\n",
    "   \"\"\"\n",
    "  \n",
    "   generated_code = get_llm_response(\"groq\", code_generation_prompt)\n",
    "  \n",
    "   print(f\"\\n\\nGenerated Code: start|{generated_code}|END\\n\\n\")\n",
    "  \n",
    "   old_stdout = sys.stdout\n",
    "   sys.stdout = buffer = io.StringIO()\n",
    "  \n",
    "   exec(generated_code)\n",
    "  \n",
    "   sys.stdout = old_stdout\n",
    "   output = buffer.getvalue()\n",
    "  \n",
    "   print(f\"\\n\\n***** Execution Result: |start|{output.strip()}|end| *****\\n\\n\")\n",
    "  \n",
    "   return {\n",
    "       \"generated_code\": generated_code,\n",
    "       \"execution_result\": output.strip()\n",
    "   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executor(action: str, parameters: Dict[str, Any], user_query: str, memory: List[Dict[str, Any]]) -> Any:\n",
    "   if action == \"generate_code\":\n",
    "       print(f\"Generating code for: {parameters['prompt']}\")\n",
    "       return generate_and_execute_code(parameters[\"prompt\"], user_query, memory)\n",
    "   elif action == \"reasoning\":\n",
    "       return parameters[\"prompt\"]\n",
    "   else:\n",
    "       return f\"Action '{action}' not implemented\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autonomous_agent(user_query: str) -> List[Dict[str, Any]]:\n",
    "   memory = []\n",
    "   subtasks = planner(user_query)\n",
    "   for subtask in subtasks:\n",
    "       reasoning = reasoner(user_query, subtasks, subtask, memory)\n",
    "       action_info = actioner(user_query, subtasks, subtask, reasoning, memory)\n",
    "       execution_result = executor(action_info[\"action\"], action_info[\"parameters\"], user_query, memory)\n",
    "       evaluation = evaluator(user_query, subtasks, subtask, action_info, execution_result, memory)\n",
    "       if evaluation[\"retry\"]:\n",
    "           continue\n",
    "       memory.append({\n",
    "           \"subtask\": subtask,\n",
    "           \"result\": execution_result,\n",
    "           \"evaluation\": evaluation[\"evaluation\"]\n",
    "       })\n",
    "   final_answer = final_answer_extractor(user_query, subtasks, memory)\n",
    "   return final_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subtasks': [\"reasoning: Count the number of syllables in the word strawberry to determine the number of possible 'r's.\", \"reasoning: Determine the base form of the suffix '-ry' in the word strawberry, which may influence the number of 'r's.\", \"generate_code: Count the number of 'r's in the string in Python by using the count() method.\", 'generate_code: Use the string strawberry to call the count method and retrieve the result.', 'generate_code: Combine the results of the count() method into the final answer.']}\n",
      "Generating code for: Use the string 'strawberry' to call the count() method and retrieve the result.\n",
      "\n",
      "\n",
      "Generated Code: start|strawberry = \"strawberry\"\n",
      "from collections import Counter\n",
      "\n",
      "chr_var = 'r'\n",
      "\n",
      "result = Counter(strawberry)[chr_var]\n",
      "print(result)|END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***** Execution Result: |start|3|end| *****\n",
      "\n",
      "\n",
      "Generating code for: Use the string count() method to count the number of 'r's in the string 'strawberry'.\n",
      "\n",
      "\n",
      "Generated Code: start|strawberry = \"strawberry\"\n",
      "from collections import Counter\n",
      "\n",
      "chr_var = 'r'\n",
      "\n",
      "result = Counter(strawberry)[chr_var]\n",
      "print(result)|END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***** Execution Result: |start|3|end| *****\n",
      "\n",
      "\n",
      "Generating code for: Modify the existing code to print or return the result of the count() method to provide the final answer to the user inquiry about the number of 'r's in the string 'strawberry'.\n",
      "\n",
      "\n",
      "Generated Code: start|strawberry = \"strawberry\"\n",
      "from collections import Counter\n",
      "\n",
      "chr_var = 'r'\n",
      "\n",
      "result = Counter(strawberry)[chr_var]\n",
      "print(result)|END\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***** Execution Result: |start|3|end| *****\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = autonomous_agent(\"How many r's are in strawberry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"result\": \"3\"}'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 2 r\\'s in the word \"strawberry\".'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_llm_response(client=\"groq\",prompt=\"How many r's are in strawberry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The word \"strawberry\" contains two \\'r\\'s.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_llm_response(client=\"openai\",prompt=\"How many r's are in strawberry\",openai_model=\"gpt-4-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
